{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Entries: \n",
    "0 Bound, 1 Date + Time, 2 Seq No, 3 Lane, 4 Speed, 5 Class, 6 No of Axle, [ Axle Weight, Axle Spacing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:red\"> Methods and Procedures </span>\n",
    "\n",
    "\n",
    "#### Data Cleaning\n",
    "- Sort the data based on Date > Bound > lane   DONE\n",
    "- obtain the Gap Time    DONE\n",
    "\n",
    "#### Fitting and Regression\n",
    "- Find the daily average vehicle flow  DONE\n",
    "- Find the Variation in Traffic flow across different times of the day DONE\n",
    "- Find the variation in LDV:HDV ratio across different times of the day  GAVE UP\n",
    "- Find the variation in (Traffic Volume in hour)/(Traffic Volume in day) across different times of the day [Reference](https://medium.com/hal24k-techblog/a-guide-to-generating-probability-distributions-with-neural-networks-ffc4efacd6a4) \n",
    "- Find the PDF of the vehicle weight of LDV  DONE \n",
    "- Find the PDF of the vehicle weight of HDV  DONE\n",
    "- Find the Relation between parameters with the below methods  \n",
    "\n",
    "#### Monte Carlo Simluation\n",
    "- Set up an environment for simulation. Refer to your IR. \n",
    "- Run for 2400 years (USE SEED to make sure the value is same every time) -> maybe do it a few times and take the average?\n",
    "- Obtain max load effect. -> Transition to SAP2000\n",
    "- Then maybe measure the load effect with varying parameters to plot a 3d surface. (but what parameters should I vary with?)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between each entries: \n",
    " - Time of the day -> Proportion of Vehicle Class\n",
    "   - Vehicle Class -> Vehicle Speed ? Need to see if there is a relationship or not. \n",
    "      - Vehicle Class + Vehicle Speed -> Gap Time  \n",
    "        //\n",
    "   - Vehicle Class -> Axle Number \n",
    "      - Axle number -> Total Weight & Axle Weight\n",
    "\n",
    "Multiple Lasso/Ridge Regression or polynomial regression would be used to draw the relationships of the above figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions and Relationships: \n",
    "- the Traffic composition relates to the time of the day\n",
    "- Gap Time between vehicles is determined by the vehicle speed and the vehicle class ie. weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    all_data= pd.read_excel('output.xlsx')\n",
    "    return all_data\n",
    "# usecols='A:I'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates the Gap Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gap_distance():\n",
    "    all_data = import_data()\n",
    "    # all_data.loc[:,'Seq No':'Class'].head()\n",
    "    all_data['Date + Time'] = pd.to_datetime(all_data['Date + Time'])\n",
    "    all_data['Date'] = pd.to_datetime(all_data['Date + Time']).dt.date\n",
    "    all_data['Date'].unique()\n",
    "\n",
    "    all_data = all_data.sort_values(['Date','Bound', 'Lane','Seq No'])\n",
    "\n",
    "    all_data['Same Bound'] = all_data['Bound'] == all_data['Bound'].shift(1)\n",
    "    all_data['Same Lane'] = all_data['Lane'] == all_data['Lane'].shift(1)    \n",
    "    # if all_data.loc['Bound'].eq(all_data.loc['Bound'].shift(1)) and all_data.loc['Lane'].eq(all_data.loc['Lane'].shift(1)):\n",
    "    #     all_data['Gap Time'] = all_data['Speed'] * (all_data['Date + Time'].shift(1) - all_data['Date + Time'])\n",
    "    # else:\n",
    "    #     all_data['Gap Time'] = pd.NA\n",
    "\n",
    "    filter1 = all_data['Same Bound'] == True\n",
    "    filter2 = all_data['Same Lane'] == True\n",
    "    not_filter1 = all_data['Same Bound'] == False\n",
    "    not_filter2 = all_data['Same Lane'] == False\n",
    "\n",
    "    change = all_data['Date + Time'].diff().dt.seconds\n",
    "    # all_data['Speed'] * (all_data['Date + Time'].shift(1) - all_data['Date + Time'])\n",
    "\n",
    "    all_data.loc[filter1 & filter2, 'Gap Time'] = change\n",
    "    all_data.loc[not_filter1, 'Gap Time'] = pd.NA\n",
    "    all_data.loc[not_filter2, 'Gap Time'] = pd.NA\n",
    "\n",
    "\n",
    "    # all_data = all_data.fillna(all_data['Gap Time'].mean())\n",
    "\n",
    "    cols = ['Gap Time']\n",
    "    all_data.loc[:,cols] = all_data.loc[:,cols].bfill()\n",
    "    return \n",
    "# cal_gap_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = all_data.drop(columns=['Date + Time','Date']).loc[:,:'No of Axle']\n",
    "# Weight_Gap_Corr = subset.corr().round(2)\n",
    "# Weight_Gap_Corr = Weight_Gap_Corr.abs()\n",
    "\n",
    "# def highlight_greater_than_7(val):\n",
    "#     \"\"\"\n",
    "#     Takes a scalar and returns a string with\n",
    "#     the css property 'background-color: yellow' for\n",
    "#     values greater than 80, black otherwise.\n",
    "#     \"\"\"\n",
    "#     color = 'green' if float(val) >= 0.7 else 'grey'\n",
    "#     return f'background-color: {color}'\n",
    "\n",
    "# Weight_Gap_Corr = Weight_Gap_Corr.style.applymap(highlight_greater_than_7)\n",
    "\n",
    "\n",
    "# Weight_Gap_Corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there is not much relationship between Gap distace and Total Weight\n",
    "\n",
    "There is relationship between speed and lane number, but should not affect the calculation of load effect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 1 - Motor cycles\n",
    "\n",
    "Class 2 - Private cars\n",
    "\n",
    "\n",
    "Class 3 - Light buses\n",
    "\n",
    "Class 4 - Light Goods Vehicles\n",
    "\n",
    "Class 5 - Medium Goods Vehicles\n",
    "\n",
    "Class 6 - Rigid Heavy Goods Vehicles\n",
    "\n",
    "Class 7 - Articulated Heavy Goods Vehicles\n",
    "\n",
    "Class 8 - Buses and Coaches\n",
    "\n",
    "Class 9 - Unclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Vehicular Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_veh_flow():\n",
    "    date_list = all_data['Date'].unique()\n",
    "    avf_list = []\n",
    "    for date in date_list: \n",
    "        avf_list.append(round(len(all_data[all_data['Date']==date].index),2))\n",
    "    dict = {'Date': date_list, \"Daily Vehicular Flow\": avf_list}\n",
    "    AVFdf = pd.DataFrame(dict)\n",
    "    print(AVFdf)\n",
    "    mean_vf = AVFdf[\"Daily Vehicular Flow\"].mean()\n",
    "    sd_vf = AVFdf[\"Daily Vehicular Flow\"].std()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot_speed_and_gap_distance():\n",
    "    date_list = all_data['Date'].unique()\n",
    "\n",
    "    from statsmodels.graphics.boxplots import violinplot\n",
    "    \n",
    "    speed_list = []\n",
    "    gap_list = []\n",
    "    for date in date_list:\n",
    "        speed_list.append(np.array(all_data[all_data['Date']==date]['Speed']))\n",
    "        gap_list.append(np.array(all_data[all_data['Date']==date]['Gap Time']))\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    violinplot(data=speed_list, ax=ax1, labels=date_list, side='right', show_boxplot=False)\n",
    "    violinplot(data=gap_list, ax=ax2, labels=date_list, side='right', show_boxplot=False)\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Speed (km/h)')\n",
    "    plt.setp(ax1, ylim=[0,150], xlabel = 'Date', ylabel = 'Speed (km/h)')\n",
    "    # plt.setp(ax2, ylim=[0,20000], xlabel = 'Date', ylabel = 'Gap Time (m)')\n",
    "    plt.suptitle('Violin Plot of Speed and Gap Time')\n",
    "    fig.tight_layout()\n",
    "    fig.autofmt_xdate()    \n",
    "\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    violinplot(data=speed_list,ax=ax1 , labels=date_list, side='right', show_boxplot=False)\n",
    "    plt.title('Violin Plot of Speed Against Different Days')\n",
    "    plt.setp(ax1, ylim=[0,150], xlabel = 'Date', ylabel = 'Speed (km/h)')\n",
    "    fig.tight_layout()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    violinplot(data=gap_list, ax=ax1, labels=date_list, side='right', show_boxplot=False)\n",
    "    plt.setp(ax1, ylim=[0,20000], xlabel = 'Date', ylabel = 'Gap Time (m)')\n",
    "    plt.title('Violin Plot of Gap Time Against Different Days')\n",
    "    fig.tight_layout()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vehicular Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This assumes that all data are measured from the same bridge. Since for different bridges, the number of lanes would affect the expected vehicular flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in Unit of Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time as t\n",
    "def generate_time(time_list):\n",
    "    for h in range(24):\n",
    "        for m in range(60):\n",
    "            time_list.append(t(h,m).strftime(\"%H:%M\"))\n",
    "    return\n",
    "\n",
    "\n",
    "def vehicular_flow_in_minutes_setup():\n",
    "    # Create the Time column, where it is in the string format of hour:minute in 1 minute increments\n",
    "    all_data['Time'] = all_data['Date + Time'].dt.round('1min')\n",
    "    all_data['Time'] = all_data['Time'].dt.strftime(\"%H:%M\")\n",
    "\n",
    "    # Make a time list to store all the x data from 00:00 to 23:59\n",
    "    time_list = []\n",
    "    generate_time(time_list)\n",
    "    dict = {\"Time\": time_list}\n",
    "\n",
    "    # Loops over the number of recorded dates\n",
    "    total_entries = 0\n",
    "    for date in date_list:\n",
    "        vehicle_flow = []\n",
    "        # Loops over each minute\n",
    "        for time in time_list:\n",
    "            # Tallies the number of vehicle that is recorded in that minute. -> convert the unit from veh/min to veh/hr\n",
    "            # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "            vehicle_flow.append(len(all_data[(all_data['Date']==date) & (all_data['Time']==time)].index) * 60)\n",
    "\n",
    "        dict[date] = vehicle_flow\n",
    "        for item in vehicle_flow:\n",
    "            total_entries += item / 60\n",
    "\n",
    "    VF_testing = pd.DataFrame(dict)\n",
    "    return VF_testing, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicular_flow_in_minutes():\n",
    "    VF_testing, time_list = vehicular_flow_in_minutes_setup()\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "    plt.xticks(time_list[::60], rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "# vehicular_flow_in_minutes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vehicular_flow_in_minutes_scatter_plot():\n",
    "    VF_testing, time_list = vehicular_flow_in_minutes_setup()\n",
    "\n",
    "    for date in date_list:\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        plt.scatter(VF_testing['Time'], tmp, label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "    plt.xticks(time_list[::60], rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "# vehicular_flow_in_minutes_scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Unit of Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time as t\n",
    "\n",
    "def vehicular_flow_in_hours_setup():\n",
    "        \n",
    "    def generate_time(time_list):\n",
    "        for h in range(24):\n",
    "            time_list.append(t(h).strftime(\"%H\"))\n",
    "        return\n",
    "\n",
    "    # Create the Time column, where it is in the string format of hour:minute in 1 minute increments\n",
    "    all_data['Time'] = all_data['Date + Time'].dt.floor('h')\n",
    "    all_data['Time'] = all_data['Time'].dt.strftime(\"%H\")\n",
    "\n",
    "    # Make a time list to store all the x data from 00:00 to 23:59\n",
    "    time_list = []\n",
    "    generate_time(time_list)\n",
    "    dict = {\"Time\": time_list}\n",
    "\n",
    "    # Loops over the number of recorded dates\n",
    "    for date in date_list:\n",
    "        vehicle_flow = []\n",
    "        # Loops over each minute\n",
    "        for time in time_list:\n",
    "            # Tallies the number of vehicle that is recorded in that minute. -> convert the unit from veh/min to veh/hr\n",
    "            # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "            tmp = all_data[(all_data['Date']==date) & (all_data['Time']==time)]\n",
    "            hourly_flow = len(tmp.index)\n",
    "            if hourly_flow < 100: \n",
    "                hourly_flow = 0\n",
    "            vehicle_flow.append(hourly_flow)\n",
    "        dict[date] = vehicle_flow\n",
    "    # print()\n",
    "    VF_testing = pd.DataFrame(dict)\n",
    "\n",
    "    # Finding the Average Values\n",
    "    VF_testing['Total'] = VF_testing.iloc[:,1:6].sum(axis=1)\n",
    "    VF_testing['Unique Dates'] = VF_testing.iloc[:,1:6].astype(bool).sum(axis=1)\n",
    "\n",
    "    VF_testing['Average'] = VF_testing['Total'] / VF_testing['Unique Dates'] \n",
    "    \n",
    "    return VF_testing, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicular_flow_in_hours():\n",
    "    from datetime import time as t\n",
    "\n",
    "    VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "    print(len(time_list))\n",
    "    # Print one with the 0 value\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Print one without 0 values\n",
    "    date_list = all_data['Date'].unique()\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.plot(VF_testing['Time'], tmp, label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def vehicular_flow_in_hours_scatter_plot():\n",
    "\n",
    "    VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "    # Scatter Plot\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.scatter(VF_testing['Time'], tmp, label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# vehicular_flow_in_hours()\n",
    "# vehicular_flow_in_hours_scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Taking the Average Value across each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_time(time_list):\n",
    "#     for h in range(24):\n",
    "#         time_list.append(t(h).strftime(\"%H\"))\n",
    "#     return\n",
    "\n",
    "# # Create the Time column, where it is in the string format of hour:minute in 1 minute increments\n",
    "# all_data['Time'] = all_data['Date + Time'].dt.floor('h')\n",
    "# all_data['Time'] = all_data['Time'].dt.strftime(\"%H\")\n",
    "\n",
    "# # Make a time list to store all the x data from 00:00 to 23:59\n",
    "# time_list = []\n",
    "# generate_time(time_list)\n",
    "# dict = {\"Time\": time_list}\n",
    "\n",
    "# vehicle_flow = []\n",
    "# # Loops over each minute\n",
    "# for time in time_list:\n",
    "#     # Tallies the number of vehicle that is recorded in that minute. -> convert the unit from veh/min to veh/hr\n",
    "#     # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "#     tmp = all_data[all_data['Time']==time]\n",
    "#     no_of_dates_involved = len(tmp['Date'].unique())\n",
    "#     if no_of_dates_involved != 0:\n",
    "#         hourly_flow_value = len(tmp.index) / no_of_dates_involved\n",
    "#     vehicle_flow.append(hourly_flow_value)\n",
    "#     dict['Vehicle Flow'] = vehicle_flow\n",
    "\n",
    "# VF_testing = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicular_flow_in_hours_with_average_and_scatter():\n",
    "\n",
    "    VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "\n",
    "    # graph plot the average value\n",
    "    plt.plot(time_list, VF_testing['Average'],'.-', label='Average')\n",
    "\n",
    "\n",
    "    # Scatter Plot\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.scatter(VF_testing['Time'], tmp, label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day (24 hour format)')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Hourly Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "# vehicular_flow_in_hours_with_average_and_scatter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining Data\n",
    "# VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "# tmp = VF_testing.dropna(subset=[\"Average\"])\n",
    "\n",
    "# # Building NN \n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# X_train       = tmp['Time'].astype(float)\n",
    "# y_train       = tmp['Average'].astype(float)\n",
    "\n",
    "# # essential preprocessing: imputation; substitute any 'NaN' with mean value \n",
    "# X_train      = X_train.fillna(X_train.mean())\n",
    "\n",
    "\n",
    "# # parameters for keras\n",
    "# input_dim        =  1 # number of neurons in the input layer\n",
    "# n_neurons        = 100       # number of neurons in the first hidden layer\n",
    "# # epochs           = 1000      # number of training cycles\n",
    "# epochs           = 1      # number of training cycles\n",
    "\n",
    "# # keras model\n",
    "# model = Sequential()        # a model consisting of successive layers\n",
    "# # input layer\n",
    "# model.add(Dense(n_neurons, input_dim=input_dim, \n",
    "#                 kernel_initializer='normal', \n",
    "#                 activation='relu'))\n",
    "# # output layer, with one neuron\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'sigmoid'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'sigmoid'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(1, kernel_initializer='normal'))\n",
    "# # compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# # train the model\n",
    "# history = model.fit(X_train,y_train, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "# # use the model to predict the prices for the test data\n",
    "# y_predicted = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the output \n",
    "# y_predicted = model.predict(X_train)\n",
    "\n",
    "# # X_train = sc.inverse_transform(X_train)\n",
    "\n",
    "# # Display the result\n",
    "# plt.plot(X_train, y_train)\n",
    "# plt.plot(X_train, y_predicted, 'r', linewidth=4)\n",
    "# plt.xticks(VF_testing['Time'].astype(int), rotation=70)\n",
    "# plt.legend(loc=0)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Time of Day (24 hour format)')\n",
    "# plt.ylabel('Vehicular Flow (veh/h)')\n",
    "# plt.title('Variations in Hourly Vehicular Flow Across Different Times of Day')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Extract the loss values from the training history\n",
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "# plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABANDONED (LDV : HDV at Varying Times of the Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(time_list[::60])\n",
    "# dict = {\"Time\": time_list}\n",
    "\n",
    "# all_data['Hour'] = all_data['Date + Time'].dt.round('1h')\n",
    "# all_data['Hour'] = all_data['Hour'].dt.strftime(\"%H:%M\")\n",
    "\n",
    "\n",
    "# for date in date_list:\n",
    "#     LDV_vehicle_flow = []\n",
    "#     HDV_vehicle_flow = []\n",
    "#     for time in time_list:\n",
    "#         # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "#         LDV_vehicle_flow.append(len(all_data[(all_data['Date']==date) & (all_data['Hour']==time) & (all_data['Class'].astype(int)<=2)].index) * 60)\n",
    "#         HDV_vehicle_flow.append(len(all_data[(all_data['Date']==date) & (all_data['Hour']==time) & (all_data['Class'].astype(int)>2)].index) * 60)\n",
    "\n",
    "#     dict[date.strftime(\"%Y%M%D\") + 'LDV'] = LDV_vehicle_flow\n",
    "#     dict[date.strftime(\"%Y%M%D\") + 'HDV'] = HDV_vehicle_flow\n",
    "\n",
    "#     print(len(vehicle_flow))\n",
    "\n",
    "#     LDV_HDV_testing = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import time as t\n",
    "\n",
    "def generate_time(time_list):\n",
    "    for h in range(24):\n",
    "        for m in range(60):\n",
    "            time_list.append(t(h,m))\n",
    "    return\n",
    "\n",
    "total_list = []\n",
    "time_list = []\n",
    "generate_time(time_list)\n",
    "\n",
    "for date in date_list:\n",
    "    # print(LDV_HDV_testing[date.strftime(\"%Y%M%D\") + 'LDV'])\n",
    "    # print(np.array(LDV_HDV_testing[date.strftime(\"%Y%M%D\") + 'LDV']))\n",
    "    total_list = np.concatenate((total_list, (np.array(LDV_HDV_testing[date.strftime(\"%Y%M%D\") + 'LDV']))))\n",
    "time_list = time_list * len(date_list)\n",
    "\n",
    "print(len(total_list))\n",
    "print(len(time_list))\n",
    "\n",
    "plt.scatter(total_list, time_list, label=date)\n",
    "plt.xticks(time_list[::60], rotation=70)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Vehicular Flow (veh/h)')\n",
    "plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDV to HDV Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LDV_to_HDV_ratio():\n",
    "    # HDV\n",
    "    HDV = all_data.loc[(all_data['Class'].astype(int) > 2)| ((all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) > 6000))]\n",
    "    total_trucks = HDV[HDV.columns[0]].count()\n",
    "    # LDV - Private Vehicle and Motorbikes\n",
    "    PV = all_data.loc[(all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) >= 1500) & (all_data['Total Weight'].astype(int) <= 6000) ]\n",
    "    # PV['Total Weight'].mode\n",
    "    total_cars = PV[PV.columns[0]].count()\n",
    "\n",
    "    # LDV - Motorbike\n",
    "    MB = all_data.loc[(all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) < 1500) ]\n",
    "    total_motorbikes = MB[MB.columns[0]].count()\n",
    "\n",
    "    print([total_trucks,total_cars,total_motorbikes])\n",
    "    print(sum([total_trucks,total_cars,total_motorbikes]))\n",
    "    return HDV, PV, MB, total_trucks,total_cars,total_motorbikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def show_pie_chart():\n",
    "    HDV, PV, MB, total_trucks,total_cars,total_motorbikes = find_LDV_to_HDV_ratio()\n",
    "    pie_chart_labels = ['HDV','LDV - cars','LDV - motorbikes']\n",
    "    myexplode = [0.2,0,0]\n",
    "\n",
    "    print([total_trucks,total_cars,total_motorbikes])\n",
    "\n",
    "    plt.pie(np.array([total_trucks,total_cars,total_motorbikes]),labels = pie_chart_labels, explode = myexplode, shadow = True, autopct='%1.0f%%')\n",
    "    plt.legend(title = \"\", bbox_to_anchor=(1,0), loc=\"lower right\", bbox_transform=plt.gcf().transFigure)\n",
    "    plt.title('Percentage of LDV to HDV')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private Vehicle Weight Distribution. referenced from [Here](https://colab.research.google.com/drive/11A5Td8nxGSbThzL0NPwwv-E5GpBa0Fv9#scrollTo=sMvpBFj5x1L0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the Private Vehicle from other data\n",
    "\n",
    "Calculated the Cumulative Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_for_PV():\n",
    "    HDV, PV, MB, total_trucks,total_cars,total_motorbikes = find_LDV_to_HDV_ratio()\n",
    "\n",
    "    PV_total_weight_and_cumper = pd.DataFrame(PV['Total Weight'].value_counts()).reset_index()\n",
    "    PV_total_weight_and_cumper.columns = ['Total Weight', 'Count']\n",
    "    PV_total_weight_and_cumper['Percentage'] = PV_total_weight_and_cumper['Count'] / total_cars\n",
    "    # Before Sorting\n",
    "    x_data1 = np.log10( PV_total_weight_and_cumper['Total Weight'])\n",
    "    x_data = PV_total_weight_and_cumper['Total Weight']\n",
    "    y_data = PV_total_weight_and_cumper['Percentage']\n",
    "    y_data1 = np.log10(PV_total_weight_and_cumper['Percentage'])\n",
    "\n",
    "\n",
    "    plt.title('Probability Density Distribution of Private Vehicle Weight')\n",
    "    plt.scatter(x_data, y_data , s=2)\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Gross Vehicle Weight (kg)')\n",
    "    plt.ylabel('Probaility Density')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # ! Sort\n",
    "    PV_total_weight_and_cumper = PV_total_weight_and_cumper.sort_values('Total Weight')\n",
    "    PV_total_weight_and_cumper['Cumulative Probability'] = PV_total_weight_and_cumper['Percentage'].cumsum()\n",
    "\n",
    "\n",
    "    plt.scatter(PV_total_weight_and_cumper['Total Weight'], PV_total_weight_and_cumper['Cumulative Probability'] , s=2)\n",
    "    plt.title('Private Vehicle Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test = train_test_split(x_data,y_data,test_size = 0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixlaw/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_model_for_private_car():\n",
    "    # PV_total_weight_and_cumper['Total Weight'] = np.log(PV_total_weight_and_cumper['Total Weight'])\n",
    "    X_train       = PV_total_weight_and_cumper['Total Weight']\n",
    "    y_train       = PV_total_weight_and_cumper['Cumulative Probability']\n",
    "\n",
    "    # essential preprocessing: imputation; substitute any 'NaN' with mean value \n",
    "    X_train      = X_train.fillna(X_train.mean())\n",
    "\n",
    "\n",
    "    # parameters for keras\n",
    "    input_dim        =  1 # number of neurons in the input layer\n",
    "    n_neurons        =  50       # number of neurons in the first hidden layer\n",
    "    epochs           = 400       # number of training cycles\n",
    "\n",
    "    # keras model\n",
    "    model = Sequential()        # a model consisting of successive layers\n",
    "    # input layer\n",
    "    model.add(Dense(n_neurons, input_dim=input_dim, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    # output layer, with one neuron\n",
    "    model.add(Dense(n_neurons, activation = 'linear'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'tanh'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(y_train,X_train, epochs=epochs, verbose=0,validation_split=0.2)\n",
    "\n",
    "    # use the model to predict the prices for the test data\n",
    "    x_predicted = model.predict(y_train)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "def evaluate_model_private_car():\n",
    "\n",
    "    train_model_for_private_car()\n",
    "    plt=reload(plt)\n",
    "    # Display the result\n",
    "    plt.scatter(X_train, y_train, s=1)\n",
    "    plt.plot(x_predicted, y_train, 'r', linewidth=4)\n",
    "    plt.title('Private Vehicle Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Extract the loss values from the training history\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt=reload(plt)\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=300)\n",
    "\n",
    "# plt.plot(history.history['mean_squared_error'])\n",
    "# plt.plot(history.history['val_mean_squared_error'])\n",
    "# plt.title('Model Mean Squared Error')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDV Weight Distribution. referenced from [Here](https://colab.research.google.com/drive/11A5Td8nxGSbThzL0NPwwv-E5GpBa0Fv9#scrollTo=sMvpBFj5x1L0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_for_HDV():\n",
    "\n",
    "    HDV, PV, MB, total_trucks,total_cars,total_motorbikes = find_LDV_to_HDV_ratio()\n",
    "\n",
    "    HDV_total_weight_and_cumper = pd.DataFrame(HDV['Total Weight'].value_counts()).reset_index()\n",
    "    HDV_total_weight_and_cumper.columns = ['Total Weight', 'Count']\n",
    "    HDV_total_weight_and_cumper['Percentage'] = HDV_total_weight_and_cumper['Count'] / total_trucks\n",
    "    # Sort it by Weight First or it messes up the Cumu Prob\n",
    "    HDV_total_weight_and_cumper = HDV_total_weight_and_cumper.sort_values(\"Total Weight\")\n",
    "    HDV_total_weight_and_cumper['Cumulative Probability'] = HDV_total_weight_and_cumper['Percentage'].cumsum()\n",
    "    truck_x_data = HDV_total_weight_and_cumper['Total Weight']\n",
    "    truck_y_data = HDV_total_weight_and_cumper['Percentage']\n",
    "\n",
    "\n",
    "    plt=reload(plt)\n",
    "\n",
    "    plt.scatter(truck_x_data, truck_y_data , s=2)\n",
    "    plt.title('HDV Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    truck_x_data = HDV['Total Weight']\n",
    "    truck_y_data = HDV_total_weight_and_cumper['Count']\n",
    "\n",
    "\n",
    "    plt=reload(plt)\n",
    "\n",
    "    plt.hist(x=truck_x_data,bins=1000, density=True)\n",
    "    # truck_x_data = HDV_total_weight_and_cumper['Total Weight']\n",
    "    # plt.scatter(truck_x_data, truck_y_data , s=2)\n",
    "    plt.title('HDV Gross Vehicle Weight Probability Density Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight (kg)')\n",
    "    plt.ylabel('Probabilty Density (%)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(list(HDV_total_weight_and_cumper))\n",
    "\n",
    "\n",
    "    truck_x_data = HDV_total_weight_and_cumper['Total Weight']\n",
    "    truck_y_data = HDV_total_weight_and_cumper['Cumulative Probability']\n",
    "    plt.scatter(truck_x_data, truck_y_data , s=2)\n",
    "    plt.title('HDV Gross Vehicle Weight Cumulative Probability Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight (kg)')\n",
    "    plt.ylabel('Cumulative Probability (%)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model \n",
    "# Building NN \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_model_for_HDV():\n",
    "    \n",
    "    X_train       = truck_x_data\n",
    "    y_train       = truck_y_data\n",
    "\n",
    "    # essential preprocessing: imputation; substitute any 'NaN' with mean value \n",
    "    X_train      = X_train.fillna(X_train.mean())\n",
    "\n",
    "\n",
    "    # parameters for keras\n",
    "    input_dim        =  1 # number of neurons in the input layer\n",
    "    n_neurons        =  50       # number of neurons in the first hidden layer\n",
    "    epochs           = 400       # number of training cycles\n",
    "\n",
    "    # keras model\n",
    "    model = Sequential()        # a model consisting of successive layers\n",
    "    # input layer\n",
    "    model.add(Dense(n_neurons, input_dim=input_dim, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    # output layer, with one neuron\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'tanh'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(y_train,X_train, epochs=epochs, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # use the model to predict the prices for the test data\n",
    "    x_predicted = model.predict(y_train)\n",
    "\n",
    "    # ! Gotta Return some parameters\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# model.fit( truck_y_data, truck_x_data, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output \n",
    "# truck_x_predicted = model.predict(truck_y_data)\n",
    "\n",
    "def evaluate_model_HDV():\n",
    "    train_model_for_HDV()\n",
    "    \n",
    "    # Display the result\n",
    "    plt.scatter(truck_x_data[::1], truck_y_data[::1], s=1)\n",
    "    plt.plot(x_predicted, truck_y_data, 'r', linewidth=4)\n",
    "    plt.title('HDV Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Extract the loss values from the training history\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt=reload(plt)\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Class and Axle Weight and Spacing Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "def axle_distribution_for_each_class_plot():\n",
    "    plt=reload(plt)\n",
    "    X = []\n",
    "    all_vehicle_classes = sorted(all_data['Class'].unique())\n",
    "    diff_no_of_axles = sorted(all_data['No of Axle'].unique())\n",
    "\n",
    "    for vehicle_class in all_vehicle_classes:\n",
    "        tmp = all_data[all_data['Class']== vehicle_class][['Total Weight','Gap Time','No of Axle']]\n",
    "        X.append(np.array(tmp['No of Axle']))\n",
    "    labels = ['Class '+ str(x) for x in all_vehicle_classes]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    plt.tight_layout()\n",
    "    plt.hist(X,bins=all_vehicle_classes, density=True, histtype='bar', label=labels)\n",
    "    plt.title('Density plot for No of Axles in Each Vehicle Class')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.xlabel('Number of Axles')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def axle_distribution_for_each_class() -> pd.DataFrame:\n",
    "    from collections import Counter\n",
    "\n",
    "    X = []\n",
    "    all_vehicle_classes = sorted(all_data['Class'].unique())\n",
    "    diff_no_of_axles = [1,2,3,4,5,6,7]\n",
    "\n",
    "    for vehicle_class in all_vehicle_classes:\n",
    "        tmp = all_data[all_data['Class']== vehicle_class][['Total Weight','Gap Time','No of Axle']]\n",
    "        X.append(np.array(tmp['No of Axle']))\n",
    "\n",
    "    df_no_of_axle = pd.DataFrame({'No of Axle':diff_no_of_axles})\n",
    "    for i in range(len(X)):\n",
    "        df_no_of_axle[all_vehicle_classes[i]] = Counter(X[i])\n",
    "        df_no_of_axle[all_vehicle_classes[i]] = df_no_of_axle[all_vehicle_classes[i]] / df_no_of_axle[all_vehicle_classes[i]].sum()\n",
    "    df_no_of_axle = df_no_of_axle.fillna(0)\n",
    "    return df_no_of_axle\n",
    "# axle_distribution_for_each_class()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding classes within HDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#within all_data obtain subset with only HDV, with information about the class value -> count all the class values occurances and output as a distribution\n",
    "# HDV -> Class\n",
    "def finding_HDV_class_distribution(all_data):\n",
    "    from collections import Counter\n",
    "\n",
    "    HDV = all_data.loc[(all_data['Class'].astype(int) > 2)| ((all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) > 6000))]\n",
    "    all_vehicle_classes = sorted(HDV['Class'].unique())\n",
    "    print(all_vehicle_classes)\n",
    "\n",
    "    output = pd.DataFrame({'Class': all_vehicle_classes})\n",
    "\n",
    "    # for i_class in all_vehicle_classes:\n",
    "    tmp = Counter(HDV['Class'])\n",
    "    output['HDV'] = tmp\n",
    "    output['HDV'] = output['HDV'].fillna(0)\n",
    "\n",
    "    # finding the probability. \n",
    "    sum = output['HDV'].sum()\n",
    "    output['Probabililty'] = output['HDV'] / sum\n",
    "    \n",
    "    output = np.array(output['Probabililty'])\n",
    "\n",
    "    return output\n",
    "# finding_HDV_class_distribution(all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the distribution of the a discrete parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within Key -> GVW\n",
    "def finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key):\n",
    "\n",
    "    # HDV = all_data.loc[(all_data[key].astype(int) == class_number) | (all_data[key].astype(str) == class_number)]\n",
    "    # all_types_within_table = sorted(HDV[key].unique())\n",
    "\n",
    "    all_types_within_class_number = sorted(HDV[class_number].unique())\n",
    "    output = pd.DataFrame({class_number: all_types_within_class_number})\n",
    "\n",
    "    # for i_class in all_vehicle_classes:\n",
    "    output[class_number] = Counter(df[class_number])\n",
    "    output[class_number] = output[class_number].fillna(0)\n",
    "\n",
    "    # finding the probability. \n",
    "    sum = output[class_number].sum()\n",
    "    output['Probabililty'] = output[class_number] / sum\n",
    "    \n",
    "    output = np.array(output['Probabililty'])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Function Template for finding distribution from dataset, plotting the dataset, and interpolating data\n",
    "\n",
    "from importlib import reload\n",
    "import random\n",
    "\n",
    "def finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key):\n",
    "    # making a subset dataframe for the dataset that suit the selection\n",
    "    tmp = all_data[all_data[vehicle_class_key] == vehicle_class_value]\n",
    "    tmp = pd.DataFrame(tmp[total_weight_key].value_counts())\n",
    "\n",
    "    tmp = tmp.sort_values(total_weight_key).reset_index()\n",
    "\n",
    "    tmp.columns = [total_weight_key, 'Count']\n",
    "    total_rows = tmp['Count'].sum()\n",
    "    \n",
    "    # calculate the percentage and cumulative probability for each dataset\n",
    "    tmp['Percentage'] = tmp['Count'] / total_rows\n",
    "    tmp['Cumulative Probability'] = tmp['Percentage'].cumsum()\n",
    "    return tmp[[total_weight_key, 'Cumulative Probability']]\n",
    "\n",
    "def plot_distribution(df:pd.DataFrame):\n",
    "    import matplotlib.pyplot as plt \n",
    "    x_axis = df.columns[0]\n",
    "    y_axis = df.columns[-1]\n",
    "    plt.scatter(df[x_axis], df[y_axis], s=2)\n",
    "    # plt.plot(df[x_axis], df[y_axis])\n",
    "    plt=reload(plt)\n",
    "    plt.title(f'{x_axis} against {y_axis}')\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def generate_and_interpolate_value(dataframe: pd.DataFrame):\n",
    "    x_axis = dataframe.columns[0]\n",
    "    y_axis = dataframe.columns[-1]\n",
    "\n",
    "    # Generate a random number between 0 and 1\n",
    "    random_prob = random.random()\n",
    "    # print(f'{random_prob = }')\n",
    "\n",
    "    # Sort the dataframe by cumulative probability column\n",
    "    sorted_df = dataframe.sort_values(y_axis)\n",
    "\n",
    "    # Find the closest value values based on the random probability\n",
    "    closest_values = []\n",
    "    closest_values.append(sorted_df[(sorted_df[y_axis] <= random_prob)].tail(1)[x_axis].values[0])\n",
    "    closest_values.append(sorted_df[(sorted_df[y_axis] >= random_prob)].tail(1)[x_axis].values[0])\n",
    "    # print(sorted_df[(sorted_df[y_axis] <= random_prob)].tail(1))\n",
    "    # print(sorted_df[(sorted_df[y_axis] >= random_prob)].head(1))\n",
    "    # print(closest_values)\n",
    "\n",
    "    # Calculate the value value using linear interpolation\n",
    "    interpolated_value = closest_values[0] + (closest_values[1] - closest_values[0]) * random_prob\n",
    "\n",
    "    print(f'{random_prob = }, {interpolated_value = }')\n",
    "\n",
    "    return interpolated_value\n",
    "\n",
    "# Usage:  \n",
    "        # tmp = finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key)\n",
    "\n",
    "# vehicle_class_key = 'Class'\n",
    "# vehicle_class_value = 2\n",
    "# total_weight_key = 'Total Weight'\n",
    "# tmp = finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key)\n",
    "# plot_distribution(tmp)\n",
    "# generate_and_interpolate_value(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns that are unnamed (Imported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(11,27):\n",
    "#     num = (i - 10) // 2\n",
    "#     if i % 2 == 0:\n",
    "#         new_column_name = 'Axle Spacing ' + str(num)\n",
    "#     else:\n",
    "#         new_column_name = 'Axle Weight ' + str(num)\n",
    "#     column_name = 'Unnamed: ' + str(i)\n",
    "#     all_data.rename(columns={column_name: new_column_name}, inplace=True)\n",
    "# all_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Traffic Flow Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = pd.read_csv('Lantau Link Yearly Traffic.csv').astype(int)\n",
    "# data = data.transpose()\n",
    "# data = data.reset_index()\n",
    "\n",
    "# data = data.rename(columns={'index':'traffic_flow'})\n",
    "# data['Year'] = range(1997,2023)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Load the traffic data from a CSV file\n",
    "# data = pd.read_csv('Lantau Link Yearly Traffic.csv').astype(int)\n",
    "# data = data.transpose()\n",
    "# data = data.reset_index()\n",
    "\n",
    "# data = data.rename(columns={'index':'traffic_flow'})\n",
    "# data['Year'] = range(1997,2023)\n",
    "# print(data)\n",
    "\n",
    "# for j in range(5,15):\n",
    "#     # Preprocess the data\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaled_data = scaler.fit_transform(data['traffic_flow'].values.reshape(-1, 1))\n",
    "\n",
    "#     # Split the data into training and testing sets\n",
    "#     train_size = int(len(scaled_data) * 0.8)\n",
    "#     train_data = scaled_data[:train_size]\n",
    "#     test_data = scaled_data[train_size:]\n",
    "#     print(f'{train_data = }, {test_data = }')\n",
    "\n",
    "#     # Define the number of time steps to use for prediction\n",
    "#     time_steps = j\n",
    "\n",
    "#     # Function to create input sequences and corresponding labels\n",
    "#     def create_sequences(data, time_steps):\n",
    "#         X, y = [], []\n",
    "#         for i in range(len(data) - time_steps):\n",
    "#             X.append(data[i:i+time_steps])\n",
    "#             y.append(data[i+time_steps])\n",
    "#         print(len(X),len(y))\n",
    "#         return np.array(X), np.array(y)\n",
    "\n",
    "#     # Create training and testing sequences\n",
    "#     X_train, y_train = create_sequences(train_data, time_steps)\n",
    "#     X_test, y_test = create_sequences(test_data, time_steps)\n",
    "\n",
    "#     # Build the LSTM model\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, input_shape=(time_steps, 1)))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#     # Set up early stopping to prevent overfitting\n",
    "#     # early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     print(f'{X_train =}')\n",
    "#     print(f'{y_train =}')\n",
    "#     model.fit(X_train, y_train, epochs=100)\n",
    "#     # model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "#     # Make predictions on the test set\n",
    "#     predicted_data = model.predict(X_test)\n",
    "\n",
    "#     # Inverse transform the predictions and the actual values\n",
    "#     predicted_data = scaler.inverse_transform(predicted_data)\n",
    "#     y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "#     # Calculate the root mean squared error (RMSE)\n",
    "#     rmse = np.sqrt(np.mean((predicted_data - y_test) ** 2))\n",
    "#     print(f\"{time_steps = }Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection for the Lantau Link Traffic in 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1997     1998     1999     2000     2001     2002     2003     2004\n",
      "      2005     2006     2007     2008     2009     2010     2011     2012\n",
      "      2013     2014     2015     2016     2017     2018     2019     2020\n",
      "      2021     2022]\n",
      " [ 2066873  9405440 13600030 13852104 13877867 15030028 14799159 17553296\n",
      "  18793501 19783091 20784378 20651031 20063672 22672072 23923671 25139219\n",
      "  26709653 28386792 30651688 32121743 34077056 35327648 35560953 19849136\n",
      "  17686845 18825001]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def load_output_from_csv(filename: str):\n",
    "    output = pd.read_csv(filename, header=None)\n",
    "    # # output = pd.read_csv(filename, header=0)\n",
    "    # print(output)\n",
    "    # # * convert the lieral list strings back into lists\n",
    "    # # output['Axle Spacing'] = output['Axle Spacing'].apply(string_to_list)\n",
    "    # # output['Axle Weight'] = output['Axle Weight'].apply(string_to_list)\n",
    "    # # output['Vehicle Type'] = output['Vehicle Type'].astype(str)\n",
    "    # output.iloc[:,5] = output.iloc[:,5].apply(string_to_list)\n",
    "    # output.iloc[:,6] = output.iloc[:,6].apply(string_to_list)\n",
    "    # output.iloc[:,1] = output.iloc[:,1].astype(str)\n",
    "\n",
    "    # for column in output.columns:\n",
    "    #     # if column == 'Axle Spacing' or column == 'Axle Weight' or column == 'Vehicle Type':\n",
    "    #     if column == 5 or column == 6 or column == 1:\n",
    "    #         continue\n",
    "    #     else: \n",
    "    #         output[column] = output[column].astype(float)\n",
    "    # return output.to_numpy()\n",
    "# output['Axle Spacing'] = output['Axle Spacing'].apply(string_to_list)\n",
    "# output['Axle Weight'] = output['Axle Weight'].apply(string_to_list)\n",
    "# output['Vehicle Type'] = output['Vehicle Type'].astype(str)\n",
    "    # output.iloc[:,5] = output.iloc[:,5].apply(string_to_list)\n",
    "    # output.iloc[:,6] = output.iloc[:,6].apply(string_to_list)\n",
    "    # output.iloc[:,1] = output.iloc[:,1].astype(str)\n",
    "    # print(f'{range(len(output.columns))=}')\n",
    "    # print(len(output.iloc[0,:]))\n",
    "    # for row in range(len(output.iloc[0,:])-1):\n",
    "\n",
    "    #     # if row == 'Axle Spacing' or row == 'Axle Weight' or row == 'Vehicle Type':\n",
    "        # if row == 0:\n",
    "        #     output.iloc[row,:] = output.iloc[row,:].astype(int)\n",
    "    #     # elif row == 5 or row == 6:\n",
    "    #     #     # print(output.iloc[row,:].values[0])\n",
    "    #     #     # print(string_to_list(output.iloc[row,:].values[0]))\n",
    "    #     #     output.iloc[row,:] = output.iloc[row,:].apply(string_to_list)\n",
    "        # else: \n",
    "    output.iloc[0,:] = output.iloc[0,:].astype(int)\n",
    "    output.iloc[1,:] = output.iloc[1,:].astype(float)\n",
    "    \n",
    "    return output.to_numpy()\n",
    "\n",
    "LLYT = load_output_from_csv('Lantau Link Yearly Traffic.csv')\n",
    "print(LLYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[2066873].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, include_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ! Degree 2 means that the paraeter can be generated as x and x^2\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m poly_features \u001b[38;5;241m=\u001b[39m \u001b[43mpoly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ! poly_features include both x and x^2 term as the input of the regression function fitting\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# from sk_valuelearn.linear_model import LinearRegression\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# # we do this because polynomial regression is linear\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # ! the training dataset and testing dataset are not separated.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m poly_reg_model \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_polynomial.py:322\u001b[0m, in \u001b[0;36mPolynomialFeatures.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    Compute number of output features.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m        Fitted transformer.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, Integral):\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_bias:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:938\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    943\u001b[0m         )\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[2066873].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CIVL 7018\n",
    "k_value = LLYT[1].reshape(-1, 1)[0]\n",
    "q_value = LLYT[0].reshape(-1, 1)[0]\n",
    "\n",
    "# print(k_value,q_value)\n",
    "FD_data= pd.DataFrame({'k':k_value,'q':q_value})\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# ! Degree 2 means that the paraeter can be generated as x and x^2\n",
    "poly_features = poly.fit_transform(k_value)\n",
    "# ! poly_features include both x and x^2 term as the input of the regression function fitting\n",
    "# from sk_valuelearn.linear_model import LinearRegression\n",
    "# # we do this because polynomial regression is linear\n",
    "# # ! the training dataset and testing dataset are not separated.\n",
    "poly_reg_model = LinearRegression()\n",
    "poly_reg_model.fit(poly_features, q_value)\n",
    "# # predicted from fitted model\n",
    "y_predicted = poly_reg_model.predict(poly_features)\n",
    "print(y_predicted)\n",
    "FD_data['Fitted'] = y_predicted[0]\n",
    "# FD_data.head()\n",
    "# FD_data = FD_data.sort_values(by='k_value')\n",
    "## plot the predicted data\n",
    "\n",
    "FD_data = FD_data.sort_values(by='k')\n",
    "## plot the predicted data\n",
    "plt.plot(FD_data['k'], FD_data['Fitted'], 'g.-', label='Fitted curve')\n",
    "plt.plot(FD_data['k'], FD_data['q'], '.', label='Data')\n",
    "\n",
    "# plt.plot(k_value, y_predicted, 'g.-', label='Fitted curve')\n",
    "# plt.plot(k_value, q_value, '.', label='Data')\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('Density (veh/km)')\n",
    "plt.ylabel('Flow (veh/hour)')\n",
    "plt.title('q = %.2f + %.2f*k %.2f*k^2'%(\n",
    "    poly_reg_model.intercept_[0], poly_reg_model.coef_[0, 0], poly_reg_model.coef_[0, 1]\n",
    "))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
